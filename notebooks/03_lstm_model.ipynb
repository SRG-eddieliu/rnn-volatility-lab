{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Pure LSTM\n",
    "\n",
    "Train the Pure LSTM volatility model with `MSE` loss and evaluate with both `MSE` and `QLIKE`.\n",
    "This notebook checkpoints predictions/logs/gates after every split and can resume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"src\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.evaluation import evaluate_forecasts\n",
    "from src.models.rnn import RNNTrainingConfig, run_rolling_experiment\n",
    "from src.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "pd.set_option(\"display.max_columns\", 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = PROJECT_ROOT / \"data\" / \"processed\" / \"sp500_log_returns.csv\"\n",
    "splits_path = PROJECT_ROOT / \"data\" / \"processed\" / \"rolling_splits.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "splits_df = pd.read_csv(\n",
    "    splits_path,\n",
    "    parse_dates=[\n",
    "        \"train_start_date\",\n",
    "        \"train_end_date\",\n",
    "        \"val_start_date\",\n",
    "        \"val_end_date\",\n",
    "        \"test_start_date\",\n",
    "        \"test_end_date\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "pred_dir = PROJECT_ROOT / \"reports\" / \"predictions\"\n",
    "pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "pred_path = pred_dir / \"pure_lstm_predictions.csv\"\n",
    "log_path = pred_dir / \"pure_lstm_train_logs.csv\"\n",
    "gate_path = pred_dir / \"pure_lstm_gate_values.csv\"\n",
    "\n",
    "print(f\"predictions path: {pred_path}\")\n",
    "print(f\"train logs path: {log_path}\")\n",
    "print(f\"gate path: {gate_path}\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = RNNTrainingConfig(\n",
    "    lookback=21,\n",
    "    hidden_units=8,\n",
    "    dropout=0.10,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    epochs=35,\n",
    "    patience=6,\n",
    "    seed=42,\n",
    "    scale_features=True,\n",
    "    scale_target=True,\n",
    "    target_transform=\"log_standardize\",\n",
    "    log_garch_features=True,\n",
    "    eps=1e-8,\n",
    "    force_linear_output=True,\n",
    ")\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df, train_logs_df, gates_df, last_fit_history = run_rolling_experiment(\n",
    "    df=df,\n",
    "    splits_df=splits_df,\n",
    "    architecture=\"lstm\",\n",
    "    variant=\"pure\",\n",
    "    cfg=cfg,\n",
    "    output_activation=\"linear\",\n",
    "    verbose_fit=0,\n",
    "    capture_gates=True,\n",
    "    predictions_path=pred_path,\n",
    "    train_logs_path=log_path,\n",
    "    gates_path=gate_path,\n",
    "    resume=False,\n",
    "    collect_last_history=True,\n",
    ")\n",
    "\n",
    "print(f\"prediction rows: {len(predictions_df):,}\")\n",
    "print(f\"train-log rows: {len(train_logs_df):,}\")\n",
    "print(f\"gate rows: {len(gates_df):,}\")\n",
    "if last_fit_history is not None:\n",
    "    print(f\"last trained split id: {last_fit_history['split_id']}\")\n",
    "predictions_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = evaluate_forecasts(\n",
    "    predictions_df,\n",
    "    group_cols=[\"variant\", \"architecture\", \"train_loss\"],\n",
    ")\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saved predictions: {pred_path}\")\n",
    "print(f\"Saved train logs: {log_path}\")\n",
    "print(f\"Saved gate values: {gate_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting diagnostics: train vs validation loss across rolling splits.\n",
    "log_plot = train_logs_df.copy()\n",
    "required_cols = {\n",
    "    \"split_id\",\n",
    "    \"best_train_loss\",\n",
    "    \"best_val_loss\",\n",
    "    \"best_gap_val_minus_train\",\n",
    "    \"final_train_loss\",\n",
    "    \"final_val_loss\",\n",
    "    \"final_gap_val_minus_train\",\n",
    "}\n",
    "missing = sorted(required_cols.difference(log_plot.columns))\n",
    "if missing:\n",
    "    print(\n",
    "        \"Train log is missing aggregate diagnostics columns. \"\n",
    "        \"Using last trained split history instead. \"\n",
    "        f\"Missing: {missing}\"\n",
    "    )\n",
    "else:\n",
    "    log_plot = log_plot.sort_values(\"split_id\").reset_index(drop=True)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "    axes[0].plot(log_plot[\"split_id\"], log_plot[\"best_train_loss\"], label=\"Best Train Loss\", linewidth=1.2)\n",
    "    axes[0].plot(log_plot[\"split_id\"], log_plot[\"best_val_loss\"], label=\"Best Val Loss\", linewidth=1.2)\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Best Loss by Rolling Split\")\n",
    "    axes[0].grid(alpha=0.2)\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(log_plot[\"split_id\"], log_plot[\"best_gap_val_minus_train\"], label=\"Best Gap (Val - Train)\", linewidth=1.2)\n",
    "    axes[1].axhline(0.0, color=\"gray\", linestyle=\"--\", linewidth=1.0)\n",
    "    axes[1].set_xlabel(\"Split ID\")\n",
    "    axes[1].set_ylabel(\"Gap\")\n",
    "    axes[1].set_title(\"Overfit Gap by Rolling Split (Best Epoch)\")\n",
    "    axes[1].grid(alpha=0.2)\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Last trained split epoch curve (most direct overfit inspection for rolling setup).\n",
    "if last_fit_history is None:\n",
    "    print(\n",
    "        \"No last split history captured. \"\n",
    "        \"If resume=True and no new split was trained, set resume=False (or clear outputs) and rerun.\"\n",
    "    )\n",
    "else:\n",
    "    hist_df = pd.DataFrame(\n",
    "        {\n",
    "            \"epoch\": range(1, len(last_fit_history[\"loss\"]) + 1),\n",
    "            \"train_loss\": last_fit_history[\"loss\"],\n",
    "            \"val_loss\": last_fit_history[\"val_loss\"],\n",
    "        }\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    ax.plot(hist_df[\"epoch\"], hist_df[\"train_loss\"], label=\"Train Loss\", linewidth=1.4)\n",
    "    ax.plot(hist_df[\"epoch\"], hist_df[\"val_loss\"], label=\"Validation Loss\", linewidth=1.4)\n",
    "    ax.set_title(f\"Last Split Loss Curves (split_id={last_fit_history['split_id']})\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.grid(alpha=0.2)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}