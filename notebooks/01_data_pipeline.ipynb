{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Data Pipeline\n",
    "\n",
    "This notebook builds a reproducible S&P 500 daily dataset (2003-2024) for volatility modeling.\n",
    "\n",
    "Pipeline outputs:\n",
    "- Raw OHLCV data (`data/raw/sp500_raw.csv`)\n",
    "- Processed log-return dataset (`data/processed/sp500_log_returns.csv`)\n",
    "- Rolling split registry (`data/processed/rolling_splits.csv`)\n",
    "\n",
    "Design constraints:\n",
    "- Log returns as the core target input\n",
    "- Strict time-ordered splits to avoid lookahead bias\n",
    "- Deterministic transformations for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"src\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.data import PipelineConfig, generate_rolling_splits, run_data_pipeline\n",
    "from src.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.width\", 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized config keeps experiments consistent across notebooks.\n",
    "TICKER = \"^GSPC\"\n",
    "START_DATE = \"2003-01-01\"\n",
    "END_DATE = \"2024-12-31\"\n",
    "\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "config = PipelineConfig(\n",
    "    ticker=TICKER,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    raw_dir=RAW_DIR,\n",
    "    processed_dir=PROCESSED_DIR,\n",
    ")\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df, processed_df, raw_path, processed_path = run_data_pipeline(config)\n",
    "\n",
    "print(f\"Raw rows: {len(raw_df):,}\")\n",
    "print(f\"Processed rows: {len(processed_df):,}\")\n",
    "print(f\"Raw file: {raw_path}\")\n",
    "print(f\"Processed file: {processed_path}\")\n",
    "\n",
    "processed_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data-quality checks for research reproducibility.\n",
    "assert processed_df[\"date\"].is_monotonic_increasing, \"Dates must be sorted ascending.\"\n",
    "assert processed_df[\"date\"].is_unique, \"Dates must be unique.\"\n",
    "assert processed_df[\"log_return\"].notna().all(), \"Log returns must be non-null after cleaning.\"\n",
    "assert (processed_df[\"adj_close\"] > 0).all(), \"Adjusted close must be positive.\"\n",
    "\n",
    "summary = processed_df[[\"log_return\", \"sq_return\", \"rv_21d\"]].describe().T\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_df = generate_rolling_splits(\n",
    "    processed_df,\n",
    "    min_train_size=756,  # ~3 years\n",
    "    val_size=252,        # ~1 year\n",
    "    test_size=21,        # ~1 month test block\n",
    "    step_size=21,        # monthly rolling step (non-overlap with test block)\n",
    "    expanding_train=True,\n",
    ")\n",
    "\n",
    "splits_path = PROCESSED_DIR / \"rolling_splits.csv\"\n",
    "splits_df.to_csv(splits_path, index=False)\n",
    "\n",
    "print(f\"Generated {len(splits_df)} rolling splits\")\n",
    "print(f\"Saved split registry: {splits_path}\")\n",
    "splits_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "axes[0].plot(processed_df[\"date\"], processed_df[\"adj_close\"], color=\"tab:blue\")\n",
    "axes[0].set_title(\"S&P 500 Adjusted Close\")\n",
    "axes[0].set_ylabel(\"Price\")\n",
    "axes[0].grid(alpha=0.25)\n",
    "\n",
    "axes[1].plot(processed_df[\"date\"], processed_df[\"log_return\"], color=\"tab:red\", linewidth=0.8)\n",
    "axes[1].set_title(\"Daily Log Returns\")\n",
    "axes[1].set_ylabel(\"Log Return\")\n",
    "axes[1].set_xlabel(\"Date\")\n",
    "axes[1].grid(alpha=0.25)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "\n",
    "Use `data/processed/sp500_log_returns.csv` and `data/processed/rolling_splits.csv` as the common input contract for:\n",
    "- `02_garch_baseline.ipynb`\n",
    "- `03_lstm_model.ipynb`\n",
    "- `04_gru_model.ipynb`\n",
    "- `05_hybrid_models.ipynb`\n",
    "- `06_evaluation_qlike.ipynb`\n",
    "- `07_gate_visualization.ipynb`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
