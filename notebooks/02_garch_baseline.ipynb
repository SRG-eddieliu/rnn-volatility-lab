{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 GARCH Baseline (GARCH(1,1)-t)\n",
    "\n",
    "Build the benchmark volatility model using an expanding-window GARCH(1,1)-t process.\n",
    "\n",
    "Key properties:\n",
    "- One-step-ahead variance forecast\n",
    "- Strict no-lookahead implementation\n",
    "- Evaluation with both `MSE` and `QLIKE`\n",
    "- Rolling out-of-sample metrics using split definitions from `01_data_pipeline.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"src\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.evaluation import evaluate_forecasts\n",
    "from src.models.garch import rolling_garch_forecast\n",
    "from src.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 130)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = PROJECT_ROOT / \"data\" / \"processed\" / \"sp500_log_returns.csv\"\n",
    "splits_path = PROJECT_ROOT / \"data\" / \"processed\" / \"rolling_splits.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path, parse_dates=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "splits_df = pd.read_csv(\n",
    "    splits_path,\n",
    "    parse_dates=[\n",
    "        \"train_start_date\",\n",
    "        \"train_end_date\",\n",
    "        \"val_start_date\",\n",
    "        \"val_end_date\",\n",
    "        \"test_start_date\",\n",
    "        \"test_end_date\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Processed rows: {len(df):,}\")\n",
    "print(f\"Rolling splits: {len(splits_df):,}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline configuration aligned with split construction.\n",
    "RETURN_COL = \"log_return\"\n",
    "TARGET_VAR_COL = \"sq_return\"\n",
    "PRED_VAR_COL = \"garch_pred_var\"\n",
    "MIN_TRAIN_SIZE = 756\n",
    "REFIT_EVERY = 21\n",
    "\n",
    "df[PRED_VAR_COL] = rolling_garch_forecast(\n",
    "    returns=df[RETURN_COL],\n",
    "    min_train_size=MIN_TRAIN_SIZE,\n",
    "    refit_every=REFIT_EVERY,\n",
    ")\n",
    "\n",
    "df[[\"date\", RETURN_COL, TARGET_VAR_COL, PRED_VAR_COL]].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prediction panel by test windows so this baseline uses the same split protocol as deep models.\n",
    "prediction_frames = []\n",
    "for split in splits_df.itertuples(index=False):\n",
    "    mask = (df[\"date\"] >= split.test_start_date) & (df[\"date\"] <= split.test_end_date)\n",
    "    panel = df.loc[mask, [\"date\", TARGET_VAR_COL, PRED_VAR_COL]].copy()\n",
    "    panel = panel.dropna(subset=[TARGET_VAR_COL, PRED_VAR_COL])\n",
    "\n",
    "    panel = panel.rename(\n",
    "        columns={\n",
    "            TARGET_VAR_COL: \"y_true_var\",\n",
    "            PRED_VAR_COL: \"y_pred_var\",\n",
    "        }\n",
    "    )\n",
    "    panel[\"split_id\"] = int(split.split_id)\n",
    "    panel[\"variant\"] = \"baseline\"\n",
    "    panel[\"architecture\"] = \"garch11_t\"\n",
    "    panel[\"train_loss\"] = \"mse\"\n",
    "    prediction_frames.append(panel)\n",
    "\n",
    "if not prediction_frames:\n",
    "    raise ValueError(\"No GARCH predictions were generated for test windows.\")\n",
    "\n",
    "predictions_df = pd.concat(prediction_frames, ignore_index=True)\n",
    "predictions_df = predictions_df.sort_values([\"split_id\", \"date\"]).reset_index(drop=True)\n",
    "print(f\"Prediction rows (split-panel): {len(predictions_df):,}\")\n",
    "predictions_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_split = evaluate_forecasts(\n",
    "    predictions_df,\n",
    "    group_cols=[\"split_id\", \"variant\", \"architecture\", \"train_loss\"],\n",
    ")\n",
    "metrics_overall = evaluate_forecasts(\n",
    "    predictions_df,\n",
    "    group_cols=[\"variant\", \"architecture\", \"train_loss\"],\n",
    ")\n",
    "\n",
    "print(\"Overall baseline metrics:\")\n",
    "display(metrics_overall)\n",
    "\n",
    "print(\"First split-level rows:\")\n",
    "metrics_by_split.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = predictions_df.groupby(\"date\", as_index=False)[[\"y_true_var\", \"y_pred_var\"]].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(plot_df[\"date\"], plot_df[\"y_true_var\"], label=\"Realized Variance (r_t^2)\", alpha=0.75)\n",
    "ax.plot(plot_df[\"date\"], plot_df[\"y_pred_var\"], label=\"GARCH Predicted Variance\", alpha=0.85)\n",
    "ax.set_title(\"GARCH(1,1)-t: Realized vs Predicted Variance\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = PROJECT_ROOT / \"reports\" / \"predictions\"\n",
    "pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pred_path = pred_dir / \"garch_baseline_predictions.csv\"\n",
    "split_metrics_path = pred_dir / \"garch_baseline_metrics_by_split.csv\"\n",
    "overall_metrics_path = pred_dir / \"garch_baseline_metrics_overall.csv\"\n",
    "\n",
    "predictions_df.to_csv(pred_path, index=False)\n",
    "metrics_by_split.to_csv(split_metrics_path, index=False)\n",
    "metrics_overall.to_csv(overall_metrics_path, index=False)\n",
    "\n",
    "print(f\"Saved predictions: {pred_path}\")\n",
    "print(f\"Saved split metrics: {split_metrics_path}\")\n",
    "print(f\"Saved overall metrics: {overall_metrics_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
