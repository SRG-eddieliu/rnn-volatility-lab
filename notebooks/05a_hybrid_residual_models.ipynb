{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05a Hybrid Residual Models (GARCH + RNN on Residuals)\n",
    "\n",
    "This notebook fits residual hybrids:\n",
    "- GARCH(1,1)-t baseline variance forecast\n",
    "- Residual target: `residual_var = sq_return - garch_cond_var`\n",
    "- Train LSTM/GRU to predict residuals\n",
    "- Final variance forecast: `garch_cond_var + predicted_residual`\n",
    "\n",
    "Training is checkpointed split-by-split with resume support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"src\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.evaluation import evaluate_forecasts\n",
    "from src.models.garch import add_garch_feature\n",
    "from src.models.rnn import RNNTrainingConfig, run_rolling_experiment\n",
    "from src.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "pd.set_option(\"display.max_columns\", 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = PROJECT_ROOT / \"data\" / \"processed\" / \"sp500_log_returns.csv\"\n",
    "splits_path = PROJECT_ROOT / \"data\" / \"processed\" / \"rolling_splits.csv\"\n",
    "\n",
    "base_df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "splits_df = pd.read_csv(\n",
    "    splits_path,\n",
    "    parse_dates=[\n",
    "        \"train_start_date\",\n",
    "        \"train_end_date\",\n",
    "        \"val_start_date\",\n",
    "        \"val_end_date\",\n",
    "        \"test_start_date\",\n",
    "        \"test_end_date\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "pred_dir = PROJECT_ROOT / \"reports\" / \"predictions\"\n",
    "pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# LSTM paths\n",
    "lstm_raw_path = pred_dir / \"hybrid_residual_lstm_raw_predictions.csv\"\n",
    "lstm_pred_path = pred_dir / \"hybrid_residual_lstm_predictions.csv\"\n",
    "lstm_log_path = pred_dir / \"hybrid_residual_lstm_train_logs.csv\"\n",
    "lstm_gate_path = pred_dir / \"hybrid_residual_lstm_gate_values.csv\"\n",
    "\n",
    "# GRU paths\n",
    "gru_raw_path = pred_dir / \"hybrid_residual_gru_raw_predictions.csv\"\n",
    "gru_pred_path = pred_dir / \"hybrid_residual_gru_predictions.csv\"\n",
    "gru_log_path = pred_dir / \"hybrid_residual_gru_train_logs.csv\"\n",
    "gru_gate_path = pred_dir / \"hybrid_residual_gru_gate_values.csv\"\n",
    "\n",
    "base_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build GARCH feature and residual target without lookahead.\n",
    "residual_df = add_garch_feature(\n",
    "    base_df,\n",
    "    return_col=\"log_return\",\n",
    "    out_col=\"garch_cond_var\",\n",
    "    min_train_size=756,\n",
    "    refit_every=21,\n",
    ")\n",
    "residual_df[\"residual_var\"] = residual_df[\"sq_return\"] - residual_df[\"garch_cond_var\"]\n",
    "\n",
    "residual_data_path = PROJECT_ROOT / \"data\" / \"processed\" / \"sp500_log_returns_with_garch_residual.csv\"\n",
    "residual_df.to_csv(residual_data_path, index=False)\n",
    "print(f\"Saved residual dataset: {residual_data_path}\")\n",
    "\n",
    "residual_df[[\"date\", \"sq_return\", \"garch_cond_var\", \"residual_var\"]].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = RNNTrainingConfig(\n",
    "    lookback=21,\n",
    "    hidden_units=8,\n",
    "    dropout=0.10,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    epochs=35,\n",
    "    patience=6,\n",
    "    seed=42,\n",
    "    scale_features=True,\n",
    "    scale_target=True,\n",
    "    target_transform=\"standardize\",\n",
    "    log_garch_features=True,\n",
    "    eps=1e-8,\n",
    "    force_linear_output=True,\n",
    ")\n",
    "cfg\n",
    "\n",
    "print(\"target_transform:\", cfg.target_transform)\n",
    "if cfg.target_transform != \"standardize\":\n",
    "    raise ValueError(\"05a requires target_transform='standardize' for residual_var.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Train residual LSTM (linear output for signed residuals).\n",
    "lstm_raw_pred, lstm_logs, lstm_gates, hybrid_resid_lstm_last_history = run_rolling_experiment(\n",
    "    df=residual_df,\n",
    "    splits_df=splits_df,\n",
    "    architecture=\"lstm\",\n",
    "    variant=\"hybrid_residual\",\n",
    "    cfg=cfg,\n",
    "    target_col=\"residual_var\",\n",
    "    output_activation=\"linear\",\n",
    "    verbose_fit=0,\n",
    "    capture_gates=True,\n",
    "    predictions_path=lstm_raw_path,\n",
    "    train_logs_path=lstm_log_path,\n",
    "    gates_path=lstm_gate_path,\n",
    "    resume=False,\n",
    "    collect_last_history=True,\n",
    ")\n",
    "\n",
    "# 2) Reconstruct final variance forecasts = garch baseline + residual forecast.\n",
    "map_df = residual_df[[\"date\", \"sq_return\", \"garch_cond_var\"]].copy()\n",
    "\n",
    "lstm_pred = lstm_raw_pred.merge(map_df, on=\"date\", how=\"left\")\n",
    "lstm_pred[\"y_true_residual\"] = lstm_pred[\"y_true_var\"]\n",
    "lstm_pred[\"y_pred_residual\"] = lstm_pred[\"y_pred_var\"]\n",
    "lstm_pred[\"y_true_var\"] = lstm_pred[\"sq_return\"]\n",
    "lstm_pred[\"y_pred_var\"] = np.clip(lstm_pred[\"garch_cond_var\"] + lstm_pred[\"y_pred_residual\"], 1e-12, None)\n",
    "lstm_pred[\"variant\"] = \"hybrid_residual\"\n",
    "lstm_pred[\"architecture\"] = \"lstm\"\n",
    "\n",
    "lstm_pred = lstm_pred[[\n",
    "    \"date\",\n",
    "    \"split_id\",\n",
    "    \"variant\",\n",
    "    \"architecture\",\n",
    "    \"train_loss\",\n",
    "    \"y_true_var\",\n",
    "    \"y_pred_var\",\n",
    "    \"garch_cond_var\",\n",
    "    \"y_true_residual\",\n",
    "    \"y_pred_residual\",\n",
    "]]\n",
    "\n",
    "lstm_metrics = evaluate_forecasts(lstm_pred, group_cols=[\"variant\", \"architecture\", \"train_loss\"])\n",
    "lstm_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Train residual GRU (linear output for signed residuals).\n",
    "gru_raw_pred, gru_logs, gru_gates, hybrid_resid_gru_last_history = run_rolling_experiment(\n",
    "    df=residual_df,\n",
    "    splits_df=splits_df,\n",
    "    architecture=\"gru\",\n",
    "    variant=\"hybrid_residual\",\n",
    "    cfg=cfg,\n",
    "    target_col=\"residual_var\",\n",
    "    output_activation=\"linear\",\n",
    "    verbose_fit=0,\n",
    "    capture_gates=True,\n",
    "    predictions_path=gru_raw_path,\n",
    "    train_logs_path=gru_log_path,\n",
    "    gates_path=gru_gate_path,\n",
    "    resume=False,\n",
    "    collect_last_history=True,\n",
    ")\n",
    "\n",
    "# 2) Reconstruct final variance forecasts = garch baseline + residual forecast.\n",
    "gru_pred = gru_raw_pred.merge(map_df, on=\"date\", how=\"left\")\n",
    "gru_pred[\"y_true_residual\"] = gru_pred[\"y_true_var\"]\n",
    "gru_pred[\"y_pred_residual\"] = gru_pred[\"y_pred_var\"]\n",
    "gru_pred[\"y_true_var\"] = gru_pred[\"sq_return\"]\n",
    "gru_pred[\"y_pred_var\"] = np.clip(gru_pred[\"garch_cond_var\"] + gru_pred[\"y_pred_residual\"], 1e-12, None)\n",
    "gru_pred[\"variant\"] = \"hybrid_residual\"\n",
    "gru_pred[\"architecture\"] = \"gru\"\n",
    "\n",
    "gru_pred = gru_pred[[\n",
    "    \"date\",\n",
    "    \"split_id\",\n",
    "    \"variant\",\n",
    "    \"architecture\",\n",
    "    \"train_loss\",\n",
    "    \"y_true_var\",\n",
    "    \"y_pred_var\",\n",
    "    \"garch_cond_var\",\n",
    "    \"y_true_residual\",\n",
    "    \"y_pred_residual\",\n",
    "]]\n",
    "\n",
    "gru_metrics = evaluate_forecasts(gru_pred, group_cols=[\"variant\", \"architecture\", \"train_loss\"])\n",
    "gru_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final reconstructed predictions and artifacts.\n",
    "lstm_pred.to_csv(lstm_pred_path, index=False)\n",
    "gru_pred.to_csv(gru_pred_path, index=False)\n",
    "\n",
    "lstm_logs.to_csv(lstm_log_path, index=False)\n",
    "gru_logs.to_csv(gru_log_path, index=False)\n",
    "\n",
    "lstm_gates.to_csv(lstm_gate_path, index=False)\n",
    "gru_gates.to_csv(gru_gate_path, index=False)\n",
    "\n",
    "print(f\"Saved residual LSTM raw predictions: {lstm_raw_path}\")\n",
    "print(f\"Saved residual GRU raw predictions: {gru_raw_path}\")\n",
    "print(f\"Saved residual LSTM final predictions: {lstm_pred_path}\")\n",
    "print(f\"Saved residual GRU final predictions: {gru_pred_path}\")\n",
    "print(f\"Saved residual gate files: {lstm_gate_path}, {gru_gate_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting diagnostics: train vs validation loss across rolling splits.\n",
    "log_frames = []\n",
    "for model_name, logs in [(\"hybrid_residual_lstm\", lstm_logs), (\"hybrid_residual_gru\", gru_logs)]:\n",
    "    d = logs.copy()\n",
    "    d[\"model_name\"] = model_name\n",
    "    log_frames.append(d)\n",
    "\n",
    "log_plot = pd.concat(log_frames, ignore_index=True)\n",
    "required_cols = {\n",
    "    \"split_id\",\n",
    "    \"model_name\",\n",
    "    \"best_train_loss\",\n",
    "    \"best_val_loss\",\n",
    "    \"best_gap_val_minus_train\",\n",
    "}\n",
    "missing = sorted(required_cols.difference(log_plot.columns))\n",
    "if missing:\n",
    "    print(\n",
    "        \"Train log is missing aggregate diagnostics columns. \"\n",
    "        \"Using last trained split histories instead. \"\n",
    "        f\"Missing: {missing}\"\n",
    "    )\n",
    "else:\n",
    "    log_plot = log_plot.sort_values([\"model_name\", \"split_id\"]).reset_index(drop=True)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 9), sharex=True)\n",
    "\n",
    "    for model_name, d in log_plot.groupby(\"model_name\"):\n",
    "        axes[0].plot(d[\"split_id\"], d[\"best_train_loss\"], linewidth=1.1, label=f\"{model_name} train\")\n",
    "        axes[0].plot(d[\"split_id\"], d[\"best_val_loss\"], linewidth=1.1, linestyle=\"--\", label=f\"{model_name} val\")\n",
    "\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Best Train/Val Loss by Rolling Split\")\n",
    "    axes[0].grid(alpha=0.2)\n",
    "    axes[0].legend(ncol=2)\n",
    "\n",
    "    for model_name, d in log_plot.groupby(\"model_name\"):\n",
    "        axes[1].plot(d[\"split_id\"], d[\"best_gap_val_minus_train\"], linewidth=1.2, label=model_name)\n",
    "\n",
    "    axes[1].axhline(0.0, color=\"gray\", linestyle=\"--\", linewidth=1.0)\n",
    "    axes[1].set_xlabel(\"Split ID\")\n",
    "    axes[1].set_ylabel(\"Gap\")\n",
    "    axes[1].set_title(\"Overfit Gap by Rolling Split (Best Epoch)\")\n",
    "    axes[1].grid(alpha=0.2)\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for model_name, hist in [(\"hybrid_residual_lstm\", hybrid_resid_lstm_last_history), (\"hybrid_residual_gru\", hybrid_resid_gru_last_history)]:\n",
    "    if hist is None:\n",
    "        print(\n",
    "            f\"No last split history for {model_name}. \"\n",
    "            \"If resume=True and no new split was trained, set resume=False (or clear outputs) and rerun.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    hist_df = pd.DataFrame(\n",
    "        {\n",
    "            \"epoch\": range(1, len(hist[\"loss\"]) + 1),\n",
    "            \"train_loss\": hist[\"loss\"],\n",
    "            \"val_loss\": hist[\"val_loss\"],\n",
    "        }\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    ax.plot(hist_df[\"epoch\"], hist_df[\"train_loss\"], label=\"Train Loss\", linewidth=1.4)\n",
    "    ax.plot(hist_df[\"epoch\"], hist_df[\"val_loss\"], label=\"Validation Loss\", linewidth=1.4)\n",
    "    ax.set_title(f\"{model_name} Last Split Loss Curves (split_id={hist['split_id']})\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.grid(alpha=0.2)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}